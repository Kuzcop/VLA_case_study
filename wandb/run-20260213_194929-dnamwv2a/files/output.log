[34m[1mwandb[0m: Detected [huggingface_hub.inference, openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/

ðŸ”§ Initializing trainer...
Traceback (most recent call last):
  File "C:\Github_workspace\vlam_sensmore\llm_finetune.py", line 125, in <module>
    trainer = SFTTrainer(
              ^^^^^^^^^^^
  File "C:\conda\envs\vlam\Lib\site-packages\unsloth\trainer.py", line 408, in new_init
    original_init(self, *args, **kwargs)
  File "C:\conda\envs\vlam\Lib\site-packages\unsloth\trainer.py", line 314, in new_init
    original_init(self, *args, **kwargs)
  File "C:\Github_workspace\vlam_sensmore\unsloth_compiled_cache\UnslothSFTTrainer.py", line 1562, in __init__
    super().__init__(
  File "C:\Github_workspace\vlam_sensmore\unsloth_compiled_cache\UnslothSFTTrainer.py", line 992, in __init__
    train_dataset = self._prepare_dataset(
                    ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Github_workspace\vlam_sensmore\unsloth_compiled_cache\UnslothSFTTrainer.py", line 1154, in _prepare_dataset
    raise ValueError(
ValueError: Unsloth: The `formatting_func` should return a list of processed strings.
